{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.15.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from vae_utils import get_vector_from_label, add_vector_to_images, morph_faces\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "WEIGHT_CLIP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tt.Compose(\n",
    "    [\n",
    "        tt.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms, download=True)\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"/Users/parkermoesta/datasets/CelebA/img_align_celeba/\", transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channel_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input N x channel_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channel_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # 32x32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.d_block(features_d, features_d*2, 4, 2, 1), # out: (batch_size, features_d*2, 16, 16)\n",
    "            self.d_block(features_d*2, features_d*4, 4, 2, 1), # out: (batch_size, features_d*4, 8, 8)\n",
    "            self.d_block(features_d*4, features_d*8, 4, 2, 1), # out: (batch_size, features_d*8, 4, 4)\n",
    "            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # out: (batch_size, 1, 1, 1)\n",
    "        )\n",
    "\n",
    "    def d_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: N x z_dim x 1 x 1\n",
    "            self.gen_block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4\n",
    "            self.gen_block(features_g*16, features_g*8, 4, 2, 1), # N x f_g*8 x 8 x 8\n",
    "            self.gen_block(features_g*8, features_g*4, 4, 2, 1), # N x f_g*4 x 16 x 16\n",
    "            self.gen_block(features_g*4, features_g*2, 4, 2, 1), # N x f_g*2 x 32 x 32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g*2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ), # N x channels_img x 64 x 64\n",
    "            nn.Tanh(), # [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def gen_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels), # don't need to use bias as batchnorm's learnable parameters\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "# need to initialize weights on generator and discriminator\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers \n",
    "opt_gen = torch.optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
    "opt_critic = torch.optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# loss function\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 0/1583                   Loss D: -0.0709, loss G: 0.0147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m real \u001b[39m=\u001b[39m real\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(CRITIC_ITERATIONS):\n\u001b[0;32m---> 16\u001b[0m     noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrandn((BATCH_SIZE, Z_DIM, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     17\u001b[0m     fake \u001b[39m=\u001b[39m gen(noise)\n\u001b[1;32m     18\u001b[0m     critic_real \u001b[39m=\u001b[39m critic(real)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# flatten\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setting fixed noise for visualization\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "\n",
    "step = 0 # for printing to tensorboard\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real,_) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(real).reshape(-1) # flatten\n",
    "            critic_fake = critic(fake).reshape(-1) # flatten\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True) # retain_graph=True to prevent error\n",
    "            opt_critic.step()\n",
    "\n",
    "            for p in critic.parameters():\n",
    "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP) # clamp is used to clip the weights of the discriminator\n",
    "\n",
    "        ## Train Generator: min -E[critic(gen_fake)]\n",
    "        output = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(output) # we want to minimize the loss\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        # print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
