{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "0.15.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as tt\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# importing torch.optim and torch.nn.functional\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from vae_utils import get_vector_from_label, add_vector_to_images, morph_faces\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "# Create a SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "STEP_SIZE = 10\n",
    "STEPS = 60\n",
    "NOISE = 0.005\n",
    "ALPHA = 0.1\n",
    "GRADIENT_CLIP = 0.03\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 8192\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 60\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "# Loading x_train and x_test from MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4     6     4     0     2     7     6     1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some images\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "show(torchvision.utils.make_grid(images[:8]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j].item() for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "tensor(-1.)\n",
      "tensor(0.9922)\n"
     ]
    }
   ],
   "source": [
    "# checking the range of the pixel values\n",
    "img = images[0]\n",
    "print(img.shape)\n",
    "print(img.min())\n",
    "print(img.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# import random\n",
    "# import torch\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_samples(model, inp_imgs, steps, step_size, noise, return_img_per_step=False):\n",
    "    inp_imgs = inp_imgs.float()\n",
    "    imgs_per_step = []\n",
    "    for _ in range(steps):\n",
    "        inp_imgs += torch.randn_like(inp_imgs) * noise\n",
    "        inp_imgs = torch.clamp(inp_imgs, -1.0, 1.0)\n",
    "        inp_imgs = inp_imgs.float()\n",
    "        with torch.enable_grad():\n",
    "            inp_imgs.requires_grad = True\n",
    "            out_score = model(inp_imgs)\n",
    "        grads = torch.autograd.grad(out_score.sum(), inp_imgs)[0]\n",
    "        grads = torch.clamp(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n",
    "        inp_imgs += step_size * grads\n",
    "        inp_imgs = torch.clamp(inp_imgs, -1.0, 1.0)\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(inp_imgs.clone())\n",
    "    if return_img_per_step:\n",
    "        return torch.stack(imgs_per_step, dim=0)\n",
    "    else:\n",
    "        return inp_imgs.float()\n",
    "\n",
    "\n",
    "class ImageGenerator:\n",
    "    def __init__(self, num_img):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, model, writer):\n",
    "        model.eval()\n",
    "        start_imgs = (\n",
    "            torch.FloatTensor(np.random.uniform(size=(self.num_img, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)))\n",
    "            * 2\n",
    "            - 1\n",
    "        ).to(next(model.parameters()).device)\n",
    "\n",
    "        generated_images = generate_samples(\n",
    "            model,\n",
    "            start_imgs,\n",
    "            steps=1000,\n",
    "            step_size=STEP_SIZE,\n",
    "            noise=NOISE,\n",
    "            return_img_per_step=False,\n",
    "        )\n",
    "        generated_images = generated_images.cpu().detach().numpy()\n",
    "        # Save generated images as PNG files\n",
    "        for i, img in enumerate(generated_images):\n",
    "            save_path = f\"/Users/parkermoesta/Generative_models/Generative_models/Energy Based Models/imgs_{epoch:03d}_{i:02d}.png\"\n",
    "            # Convert NumPy array to PIL Image\n",
    "            pil_image = Image.fromarray(np.uint8((img + 1) * 0.5 * 255))\n",
    "\n",
    "            # Save the image\n",
    "            pil_image.save(save_path)\n",
    "\n",
    "        # Log generated images to TensorBoard\n",
    "        grid_images = torch.FloatTensor(generated_images).unsqueeze(1)\n",
    "        writer.add_images(\"Generated Images\", grid_images, epoch)\n",
    "\n",
    "        example_images = torch.cat(\n",
    "            random.choices(model.buffer.examples, k=10), dim=0\n",
    "        ).cpu().detach().numpy()\n",
    "        # Save example images as PNG files\n",
    "        for i, img in enumerate(example_images):\n",
    "            save_path = f\"/Users/parkermoesta/Generative_models/Generative_models/Energy Based Models/imgs_{epoch:03d}_{i:02d}.png\"\n",
    "            # Convert NumPy array to PIL Image\n",
    "            pil_image = Image.fromarray(np.uint8((img + 1) * 0.5 * 255))\n",
    "\n",
    "            # Save the image\n",
    "            pil_image.save(save_path)\n",
    "\n",
    "        # Log example images to TensorBoard\n",
    "        grid_images = torch.FloatTensor(example_images).unsqueeze(1)\n",
    "        writer.add_images(\"Example Images\", grid_images, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EBM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EBM, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(CHANNELS, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.buffer = Buffer(self.model)\n",
    "        self.alpha = ALPHA\n",
    "        self.loss_metric = AverageMeter()\n",
    "        self.reg_loss_metric = AverageMeter()\n",
    "        self.cdiv_loss_metric = AverageMeter()\n",
    "        self.real_out_metric = AverageMeter()\n",
    "        self.fake_out_metric = AverageMeter()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.examples = [\n",
    "            torch.randn((1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)) * 2 - 1\n",
    "            for _ in range(BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "    def sample_new_exmps(self, steps, step_size, noise):\n",
    "        n_new = np.random.binomial(BATCH_SIZE, 0.05)\n",
    "        rand_imgs = (torch.rand((n_new, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)) * 2 - 1)\n",
    "        old_imgs = torch.cat([random.choice(self.examples) for _ in range(BATCH_SIZE - n_new)], dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0)\n",
    "        inp_imgs = inp_imgs.float()\n",
    "        inp_imgs = generate_samples(\n",
    "            self.model, inp_imgs, steps=steps, step_size=step_size, noise=noise\n",
    "        )\n",
    "        self.examples = torch.split(inp_imgs, 1, dim=0) + self.examples\n",
    "        self.examples = self.examples[:BUFFER_SIZE]\n",
    "        return inp_imgs\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, device):\n",
    "    writer = SummaryWriter(log_dir=\"./logs\")\n",
    "    image_generator_callback = ImageGenerator(num_img=10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        model.train()\n",
    "        train_loss_meter = AverageMeter()\n",
    "\n",
    "        for batch_idx, real_imgs in enumerate(train_loader):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.train_step(real_imgs)\n",
    "\n",
    "            train_loss_meter.update(output['loss'])\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{EPOCHS}], \"\n",
    "                    f\"Batch [{batch_idx}/{len(train_loader)}], \"\n",
    "                    f\"Loss: {train_loss_meter.avg:.4f}\"\n",
    "                )\n",
    "\n",
    "        model.eval()\n",
    "        test_loss_meter = AverageMeter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for real_imgs in test_loader:\n",
    "                real_imgs = real_imgs.to(device)\n",
    "                output = model.test_step(real_imgs)\n",
    "                test_loss_meter.update(output['cdiv_loss'])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{EPOCHS}], \"\n",
    "            f\"Train Loss: {train_loss_meter.avg:.4f}, \"\n",
    "            f\"Test Loss: {test_loss_meter.avg:.4f}\"\n",
    "        )\n",
    "\n",
    "        writer.add_scalar(\"Train Loss\", train_loss_meter.avg, epoch)\n",
    "        writer.add_scalar(\"Test Loss\", test_loss_meter.avg, epoch)\n",
    "\n",
    "        image_generator_callback.on_epoch_end(epoch, model, writer)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"model_{epoch}.pt\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2ab02c750>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_loader))\n",
    "print(type(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset)\n",
    "print(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 32, 32])\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of first batch of train loader\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(images.shape), print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_divergence_loss(model, real_imgs, negative_samples):\n",
    "    real_imgs_energy = model(real_imgs).mean()\n",
    "    negative_samples_energy = model(negative_samples).mean()\n",
    "    loss = real_imgs_energy - negative_samples_energy\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatched Tensor types in NNPack convolutionOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m     writer\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     83\u001b[0m model \u001b[39m=\u001b[39m EBM()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 84\u001b[0m train_model(model, train_loader, test_loader, device)\n",
      "Cell \u001b[0;32mIn[62], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m real_imgs \u001b[39m=\u001b[39m real_imgs\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Generate negative samples\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m negative_samples \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49msample_new_exmps(STEPS, STEP_SIZE, NOISE)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     37\u001b[0m negative_samples \u001b[39m=\u001b[39m negative_samples\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[55], line 46\u001b[0m, in \u001b[0;36mBuffer.sample_new_exmps\u001b[0;34m(self, steps, step_size, noise)\u001b[0m\n\u001b[1;32m     44\u001b[0m inp_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([rand_imgs, old_imgs], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m inp_imgs \u001b[39m=\u001b[39m inp_imgs\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 46\u001b[0m inp_imgs \u001b[39m=\u001b[39m generate_samples(\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, inp_imgs, steps\u001b[39m=\u001b[39;49msteps, step_size\u001b[39m=\u001b[39;49mstep_size, noise\u001b[39m=\u001b[39;49mnoise\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(inp_imgs, \u001b[39m1\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples[:BUFFER_SIZE]\n",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(model, inp_imgs, steps, step_size, noise, return_img_per_step)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m     16\u001b[0m     inp_imgs\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     out_score \u001b[39m=\u001b[39m model(inp_imgs)\n\u001b[1;32m     18\u001b[0m grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(out_score\u001b[39m.\u001b[39msum(), inp_imgs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(grads, \u001b[39m-\u001b[39mGRADIENT_CLIP, GRADIENT_CLIP)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatched Tensor types in NNPack convolutionOutput"
     ]
    }
   ],
   "source": [
    "model = EBM().to(device)\n",
    "\n",
    "# putting train and test loader on device\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device):\n",
    "    writer = SummaryWriter(log_dir=\"./logs\")\n",
    "    image_generator_callback = ImageGenerator(num_img=10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        model.train()\n",
    "        train_loss_meter = AverageMeter()\n",
    "\n",
    "        for batch_idx, (real_imgs, _) in enumerate(train_loader):\n",
    "            real_imgs = real_imgs.float().to(device)\n",
    "\n",
    "            # Generate negative samples\n",
    "            negative_samples = model.buffer.sample_new_exmps(STEPS, STEP_SIZE, NOISE).float()\n",
    "            negative_samples = negative_samples.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = contrastive_divergence_loss(model, real_imgs, negative_samples)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_meter.update(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{EPOCHS}], \"\n",
    "                    f\"Batch [{batch_idx}/{len(train_loader)}], \"\n",
    "                    f\"Loss: {train_loss_meter.avg:.4f}\"\n",
    "                )\n",
    "\n",
    "        model.eval()\n",
    "        test_loss_meter = AverageMeter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for real_imgs in test_loader:\n",
    "                real_imgs = real_imgs.to(device)\n",
    "                output = model.test_step(real_imgs)\n",
    "                test_loss_meter.update(output['cdiv_loss'])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{EPOCHS}], \"\n",
    "            f\"Train Loss: {train_loss_meter.avg:.4f}, \"\n",
    "            f\"Test Loss: {test_loss_meter.avg:.4f}\"\n",
    "        )\n",
    "\n",
    "        writer.add_scalar(\"Train Loss\", train_loss_meter.avg, epoch)\n",
    "        writer.add_scalar(\"Test Loss\", test_loss_meter.avg, epoch)\n",
    "\n",
    "        image_generator_callback.on_epoch_end(epoch, model, writer)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"model_{epoch}.pt\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "model = EBM().to(device)\n",
    "train_model(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1792x28 and 100x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m i, (real_imgs, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[1;32m    159\u001b[0m     real_imgs \u001b[39m=\u001b[39m real_imgs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 160\u001b[0m     loss \u001b[39m=\u001b[39m ebm\u001b[39m.\u001b[39;49mtrain_step(real_imgs)\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    162\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[1;32m    163\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(trainloader)\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[19], line 118\u001b[0m, in \u001b[0;36mEBM.train_step\u001b[0;34m(self, real_imgs)\u001b[0m\n\u001b[1;32m    116\u001b[0m real_imgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(real_imgs) \u001b[39m*\u001b[39m NOISE\n\u001b[1;32m    117\u001b[0m real_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(real_imgs, \u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m fake_imgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49msample_new_exmps(\n\u001b[1;32m    119\u001b[0m     steps\u001b[39m=\u001b[39;49mSTEPS, step_size\u001b[39m=\u001b[39;49mSTEP_SIZE, noise\u001b[39m=\u001b[39;49mNOISE\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    121\u001b[0m inp_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([real_imgs, fake_imgs], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    122\u001b[0m real_out, fake_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator(inp_imgs), \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 96\u001b[0m, in \u001b[0;36mBuffer.sample_new_exmps\u001b[0;34m(self, steps, step_size, noise)\u001b[0m\n\u001b[1;32m     92\u001b[0m old_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     93\u001b[0m     random\u001b[39m.\u001b[39mchoices(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples, k\u001b[39m=\u001b[39mBATCH_SIZE \u001b[39m-\u001b[39m n_new), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m inp_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([rand_imgs, old_imgs], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m inp_imgs \u001b[39m=\u001b[39m generate_samples(\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator, inp_imgs, steps\u001b[39m=\u001b[39;49msteps, step_size\u001b[39m=\u001b[39;49mstep_size, noise\u001b[39m=\u001b[39;49mnoise\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(inp_imgs, BATCH_SIZE, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples[:BUFFER_SIZE]\n",
      "Cell \u001b[0;32mIn[19], line 35\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(model, inp_imgs, steps, step_size, noise, return_img_per_step)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m     34\u001b[0m     inp_imgs\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     out_score \u001b[39m=\u001b[39m model(inp_imgs)\n\u001b[1;32m     36\u001b[0m grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(out_score\u001b[39m.\u001b[39msum(), inp_imgs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(grads, \u001b[39m-\u001b[39mGRADIENT_CLIP, GRADIENT_CLIP)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 57\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     58\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m     59\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1792x28 and 100x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "IMAGE_SIZE = 28\n",
    "CHANNELS = 1\n",
    "STEPS = 20\n",
    "STEP_SIZE = 0.1\n",
    "NOISE = 0.1\n",
    "ALPHA = 0.1\n",
    "GRADIENT_CLIP = 1.0\n",
    "# Function to generate samples using Langevin Dynamics\n",
    "\n",
    "def generate_samples(model, inp_imgs, steps, step_size, noise, return_img_per_step=False):\n",
    "    inp_imgs = inp_imgs.float()\n",
    "    imgs_per_step = []\n",
    "    for _ in range(steps):\n",
    "        inp_imgs += torch.randn_like(inp_imgs) * noise\n",
    "        inp_imgs = torch.clamp(inp_imgs, -1.0, 1.0)\n",
    "        inp_imgs = inp_imgs.float()\n",
    "        with torch.enable_grad():\n",
    "            inp_imgs.requires_grad = True\n",
    "            out_score = model(inp_imgs)\n",
    "        grads = torch.autograd.grad(out_score.sum(), inp_imgs)[0]\n",
    "        grads = torch.clamp(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n",
    "        inp_imgs += step_size * grads\n",
    "        inp_imgs = torch.clamp(inp_imgs, -1.0, 1.0)\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(inp_imgs.clone())\n",
    "    if return_img_per_step:\n",
    "        return torch.stack(imgs_per_step, dim=0)\n",
    "    else:\n",
    "        return inp_imgs.float()\n",
    "\n",
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, IMAGE_SIZE * IMAGE_SIZE * CHANNELS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = x.view(-1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        return x\n",
    "\n",
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(CHANNELS, 64, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Define the buffer\n",
    "class Buffer:\n",
    "    def __init__(self, generator):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.examples = [\n",
    "            torch.rand(1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE) * 2 - 1\n",
    "            for _ in range(BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "    def sample_new_exmps(self, steps, step_size, noise):\n",
    "        n_new = np.random.binomial(BATCH_SIZE, 0.05)\n",
    "        rand_imgs = torch.rand(n_new, CHANNELS, IMAGE_SIZE, IMAGE_SIZE) * 2 - 1\n",
    "        old_imgs = torch.cat(\n",
    "            random.choices(self.examples, k=BATCH_SIZE - n_new), dim=0\n",
    "        )\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0)\n",
    "        inp_imgs = generate_samples(\n",
    "            self.generator, inp_imgs, steps=steps, step_size=step_size, noise=noise\n",
    "        )\n",
    "        self.examples = torch.split(inp_imgs, BATCH_SIZE, dim=0) + self.examples\n",
    "        self.examples = self.examples[:BUFFER_SIZE]\n",
    "        return inp_imgs\n",
    "\n",
    "# Define the energy-based model\n",
    "class EBM(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(EBM, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.buffer = Buffer(self.generator)\n",
    "        self.alpha = ALPHA\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "    def train_step(self, real_imgs):\n",
    "        real_imgs += torch.randn_like(real_imgs) * NOISE\n",
    "        real_imgs = torch.clamp(real_imgs, -1.0, 1.0)\n",
    "        fake_imgs = self.buffer.sample_new_exmps(\n",
    "            steps=STEPS, step_size=STEP_SIZE, noise=NOISE\n",
    "        )\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = torch.split(self.discriminator(inp_imgs), 2, dim=0)\n",
    "        cdiv_loss = torch.mean(fake_out, dim=0) - torch.mean(real_out, dim=0)\n",
    "        reg_loss = self.alpha * torch.mean(real_out**2 + fake_out**2, dim=0)\n",
    "        loss = cdiv_loss + reg_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.discriminator.parameters(), GRADIENT_CLIP)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def test_step(self, real_imgs):\n",
    "        batch_size = real_imgs.shape[0]\n",
    "        fake_imgs = torch.rand(batch_size, CHANNELS, IMAGE_SIZE, IMAGE_SIZE) * 2 - 1\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = torch.split(self.discriminator(inp_imgs), 2, dim=0)\n",
    "        cdiv = torch.mean(fake_out, dim=0) - torch.mean(real_out, dim=0)\n",
    "        return cdiv.item()\n",
    "\n",
    "# Define the data loaders\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "trainset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize the models and optimizer\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "ebm = EBM(generator, discriminator).to(device)\n",
    "ebm.optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_imgs, _) in enumerate(trainloader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        loss = ebm.train_step(real_imgs)\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {loss:.4f}\"\n",
    "            )\n",
    "    cdiv = ebm.test_step(real_imgs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CDIV: {cdiv:.4f}\")\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = generator(torch.randn(10, 100).to(device)).cpu()\n",
    "        save_image(fake_imgs, f\"./output/generated_img_{epoch+1:03d}.png\")\n",
    "        example_imgs = torch.cat(\n",
    "            random.choices(ebm.buffer.examples, k=10), dim=0\n",
    "        ).cpu()\n",
    "        save_image(example_imgs, f\"./output/example_img_{epoch+1:03d}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (2.0.1.post0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (1.13.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (0.11.4)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (0.8.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (1.24.3)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from pytorch_lightning) (4.5.0)\n",
      "Requirement already satisfied: requests in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/parkermoesta/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.13)\n"
     ]
    }
   ],
   "source": [
    "# install pytorch lightning\n",
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pytorch-lightning 2.0.1.post0\n",
      "Uninstalling pytorch-lightning-2.0.1.post0:\n",
      "  Successfully uninstalled pytorch-lightning-2.0.1.post0\n"
     ]
    }
   ],
   "source": [
    "# uninstall pytorch lightning\n",
    "!pip uninstall pytorch_lightning -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/978kqtls1gb5jnv3rvxtjpvh0000gn/T/ipykernel_95959/124085359.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "#import lightning as L\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#from lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "# Loading x_train and x_test from MNIST\n",
    "transform = transforms.Compose([\n",
    "   # transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
    "        super().__init__()\n",
    "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
    "        c_hid1 = hidden_features//2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features*2\n",
    "\n",
    "        # Series of convolutions and Swish activation functions\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4), # [16x16] - Larger padding to get 32x32 image\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1), #  [8x8]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1), # [4x4]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1), # [2x2]\n",
    "                Swish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(c_hid3*4, c_hid3),\n",
    "                Swish(),\n",
    "                nn.Linear(c_hid3, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for getting a new batch of \"fake\" images.\n",
    "        Inputs:\n",
    "            steps - Number of iterations in the MCMC algorithm\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
    "        \"\"\"\n",
    "        Function for sampling images for a given model.\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
    "            steps - Number of iterations in the MCMC algorithm.\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
    "        \"\"\"\n",
    "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "        # because we are only interested in the gradients of the input.\n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "\n",
    "        # Enable gradient calculation if not already the case\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "        # More efficient than creating a new tensor every iteration.\n",
    "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
    "\n",
    "        # List for storing generations at each step (for later analysis)\n",
    "        imgs_per_step = []\n",
    "\n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            # Part 1: Add noise to the input.\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "            # Part 2: calculate gradients for the current input.\n",
    "            out_imgs = -model(inp_imgs)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "            # Apply gradients to our current samples\n",
    "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone().detach())\n",
    "\n",
    "        # Reactivate gradients for parameters for training\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "\n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEnergyModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, img_shape, batch_size, alpha=0.1, lr=1e-4, beta1=0.0, **CNN_args):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.cnn = CNNModel(**CNN_args)\n",
    "        self.sampler = Sampler(self.cnn, img_shape=img_shape, sample_size=batch_size)\n",
    "        self.example_input_array = torch.zeros(1, *img_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Energy models can have issues with momentum as the loss surfaces changes with its parameters.\n",
    "        # Hence, we set it to 0 by default.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, 0.999))\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97) # Exponential decay over epochs\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # We add minimal noise to the original images to prevent the model from focusing on purely \"clean\" inputs\n",
    "        real_imgs, _ = batch\n",
    "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "        # Obtain samples\n",
    "        fake_imgs = self.sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "\n",
    "        # Predict energy score for all images\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
    "\n",
    "        # Calculate losses\n",
    "        reg_loss = self.hparams.alpha * (real_out ** 2 + fake_out ** 2).mean()\n",
    "        cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "        loss = reg_loss + cdiv_loss\n",
    "\n",
    "        # Logging\n",
    "        self.log('loss', loss)\n",
    "        self.log('loss_regularization', reg_loss)\n",
    "        self.log('loss_contrastive_divergence', cdiv_loss)\n",
    "        self.log('metrics_avg_real', real_out.mean())\n",
    "        self.log('metrics_avg_fake', fake_out.mean())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # For validating, we calculate the contrastive divergence between purely random images and unseen examples\n",
    "        # Note that the validation/test step of energy-based models depends on what we are interested in the model\n",
    "        real_imgs, _ = batch\n",
    "        fake_imgs = torch.rand_like(real_imgs) * 2 - 1\n",
    "\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
    "\n",
    "        cdiv = fake_out.mean() - real_out.mean()\n",
    "        self.log('val_contrastive_divergence', cdiv)\n",
    "        self.log('val_fake_out', fake_out.mean())\n",
    "        self.log('val_real_out', real_out.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, batch_size=8, vis_steps=8, num_steps=256, every_n_epochs=5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size         # Number of images to generate\n",
    "        self.vis_steps = vis_steps           # Number of steps within generation to visualize\n",
    "        self.num_steps = num_steps           # Number of steps to take during generation\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        # Skip for all other epochs\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Generate images\n",
    "            imgs_per_step = self.generate_imgs(pl_module)\n",
    "            # Plot and add to tensorboard\n",
    "            for i in range(imgs_per_step.shape[1]):\n",
    "                step_size = self.num_steps // self.vis_steps\n",
    "                imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
    "                grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1))\n",
    "                trainer.logger.experiment.add_image(f\"generation_{i}\", grid, global_step=trainer.current_epoch)\n",
    "\n",
    "    def generate_imgs(self, pl_module):\n",
    "        pl_module.eval()\n",
    "        start_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
    "        start_imgs = start_imgs * 2 - 1\n",
    "        torch.set_grad_enabled(True)  # Tracking gradients for sampling necessary\n",
    "        imgs_per_step = Sampler.generate_samples(pl_module.cnn, start_imgs, steps=self.num_steps, step_size=10, return_img_per_step=True)\n",
    "        torch.set_grad_enabled(False)\n",
    "        pl_module.train()\n",
    "        return imgs_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, num_imgs=32, every_n_epochs=5):\n",
    "        super().__init__()\n",
    "        self.num_imgs = num_imgs             # Number of images to plot\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            exmp_imgs = torch.cat(random.choices(pl_module.sampler.examples, k=self.num_imgs), dim=0)\n",
    "            grid = torchvision.utils.make_grid(exmp_imgs, nrow=4, normalize=True, range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"sampler\", grid, global_step=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, batch_size=1024):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        with torch.no_grad():\n",
    "            pl_module.eval()\n",
    "            rand_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
    "            rand_imgs = rand_imgs * 2 - 1.0\n",
    "            rand_out = pl_module.cnn(rand_imgs).mean()\n",
    "            pl_module.train()\n",
    "\n",
    "        trainer.logger.experiment.add_scalar(\"rand_out\", rand_out, global_step=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '/Users/parkermoesta/Generative_models/Generative_models/Energy Based Models/models'\n",
    "\n",
    "def train_model(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"MNIST\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"mps\",\n",
    "                         devices=1,\n",
    "                         max_epochs=60,\n",
    "                         gradient_clip_val=0.1,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor='val_contrastive_divergence'),\n",
    "                                    GenerateCallback(every_n_epochs=5),\n",
    "                                    SamplerCallback(every_n_epochs=5),\n",
    "                                    OutlierCallback(),\n",
    "                                    LearningRateMonitor(\"epoch\")\n",
    "                                   ])\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"MNIST.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = DeepEnergyModel.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = DeepEnergyModel(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = DeepEnergyModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # No testing as we are more interested in other properties\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name | Type     | Params | In sizes       | Out sizes\n",
      "---------------------------------------------------------------\n",
      "0 | cnn  | CNNModel | 77.0 K | [1, 1, 28, 28] | [1]      \n",
      "---------------------------------------------------------------\n",
      "77.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "77.0 K    Total params\n",
      "0.308     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783f969ee88a485b93b703a2a86c5cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ae0f28d8264000818f1df4526b0f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = train_model(img_shape=(1,28,28),\n",
    "                    batch_size=train_loader.batch_size,\n",
    "                    lr=1e-4,\n",
    "                    beta1=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
