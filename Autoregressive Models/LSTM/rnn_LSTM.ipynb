{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.1 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.15.1 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.15.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as tt\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from vae_utils import get_vector_from_label, add_vector_to_images, morph_faces\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "N_UNITS = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file = '/Users/parkermoesta/Generative_models/Generative_models/Autoregressive Models/LSTM/epirecipes.zip'\n",
    "extract_dir = '/Users/parkermoesta/Generative_models/Generative_models/Autoregressive Models/LSTM/'\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file to the specified directory\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "with open(\"/Users/parkermoesta/Generative_models/Generative_models/Autoregressive Models/LSTM/full_format_recipes.json\") as json_data:\n",
    "    recipe_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "filtered_data = [\n",
    "    \"Recipe for \" + x[\"title\"] + \" | \" + \" \".join(x[\"directions\"])\n",
    "    for x in recipe_data\n",
    "    if \"title\" in x\n",
    "    and x[\"title\"] is not None\n",
    "    and \"directions\" in x\n",
    "    and x[\"directions\"] is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20111 recipes loaded\n"
     ]
    }
   ],
   "source": [
    "# Count the recipes\n",
    "n_recipes = len(filtered_data)\n",
    "print(f\"{n_recipes} recipes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas  | Chop enough parsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan, covered, 5 minutes. Meanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute. Strain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve. Season with salt and pepper. Set bowl in an ice bath and cool to room temperature, stirring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top and chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar, 1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery, cornichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and 1/4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato salad, over ham.\n"
     ]
    }
   ],
   "source": [
    "example = filtered_data[9]\n",
    "print(example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas | Chop enough parsley leaves to measure 1 tablespoon ; reserve . Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan , covered , 5 minutes . Meanwhile , sprinkle gelatin over water in a medium bowl and let soften 1 minute . Strain broth through a fine - mesh sieve into bowl with gelatin and stir to dissolve . Season with salt and pepper . Set bowl in an ice bath and cool to room temperature , stirring . Toss ham with reserved parsley and divide among jars . Pour gelatin on top and chill until set , at least 1 hour . Whisk together mayonnaise , mustard , vinegar , 1 / 4 teaspoon salt , and 1 / 4 teaspoon pepper in a large bowl . Stir in celery , cornichons , and potatoes . Pulse peas with marjoram , oil , 1 / 2 teaspoon pepper , and 1 / 4 teaspoon salt in a food processor to a coarse mash . Layer peas , then potato salad , over ham . '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the punctuation, to treat them as separate 'words'\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "text_data = [pad_punctuation(x) for x in filtered_data]\n",
    "text_data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, tokenizer):\n",
    "        self.text_data = text_data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.text_data[idx])\n",
    "        tokens = tokens[:200]  # Clip to maximum length of 200\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text_str in data_iter:\n",
    "        yield tokenizer(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(text_data), max_tokens=VOCAB_SIZE, specials=['<pad>', '<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    batch = [torch.tensor([vocab[token] for token in tokens if token in vocab.get_stoi()]) for tokens in batch]\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    x = batch[:, :-1]\n",
    "    y = batch[:, 1:]\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = TextDataset(text_data, tokenizer)\n",
    "text_loader = DataLoader(text_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x): recipe for spiced green tahini sauce | pulse garlic , cilantro , parsley , cumin , fenugreek , and 2 teaspoons salt in a food processor until similar in texture to pesto . add tahini and lemon juice process 30 seconds ( mixture will be very thick and gray ) . with motor running , gradually drizzle in 3 / 4 cup water and process , adding more water to thin if needed , until sauce is light green and the consistency of sour cream . season with salt . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Target (y): for spiced green tahini sauce | pulse garlic , cilantro , parsley , cumin , fenugreek , and 2 teaspoons salt in a food processor until similar in texture to pesto . add tahini and lemon juice process 30 seconds ( mixture will be very thick and gray ) . with motor running , gradually drizzle in 3 / 4 cup water and process , adding more water to thin if needed , until sauce is light green and the consistency of sour cream . season with salt . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "---\n",
      "Input (x): recipe for twice - baked goat cheese souffles with salad | preheat oven to 375°f and butter six 3 / 4 - cup ramekins ( 3 3 / 4 by 2 inches ) . crumble goat cheese and separate eggs . in a saucepan melt butter over moderately low heat and whisk in flour . cook roux , whisking , 3 minutes and whisk in milk . bring mixture to a boil , whisking constantly , and simmer , whisking occasionally , 3 minutes . remove pan from heat and add yolks , mustard , 1 teaspoon thyme , two thirds cheese , and salt and pepper to taste , whisking until cheese is melted . transfer yolk mixture to a large bowl . in another large bowl with an electric mixer beat whites with a pinch salt until they just hold stiff peaks . stir one fourth whites into yolk mixture to lighten and fold in remaining whites and remaining cheese gently but thoroughly . divide soufflé mixture among ramekins and arrange in a large baking pan just large enough to hold them . add enough hot water to baking pan to reach halfway up sides of ramekins\n",
      "Target (y): for twice - baked goat cheese souffles with salad | preheat oven to 375°f and butter six 3 / 4 - cup ramekins ( 3 3 / 4 by 2 inches ) . crumble goat cheese and separate eggs . in a saucepan melt butter over moderately low heat and whisk in flour . cook roux , whisking , 3 minutes and whisk in milk . bring mixture to a boil , whisking constantly , and simmer , whisking occasionally , 3 minutes . remove pan from heat and add yolks , mustard , 1 teaspoon thyme , two thirds cheese , and salt and pepper to taste , whisking until cheese is melted . transfer yolk mixture to a large bowl . in another large bowl with an electric mixer beat whites with a pinch salt until they just hold stiff peaks . stir one fourth whites into yolk mixture to lighten and fold in remaining whites and remaining cheese gently but thoroughly . divide soufflé mixture among ramekins and arrange in a large baking pan just large enough to hold them . add enough hot water to baking pan to reach halfway up sides of ramekins .\n",
      "---\n",
      "Input (x): recipe for oven - poached salmon steaks with mustard dill sauce | in a saucepan combine the wine , 1 1 / 2 cups water , the sugar , the peppercorns , the coriander seeds , the mustard seeds , the salt , and the onion , simmer the mixture for 15 minutes , and let cool to room temperature . halve the salmon steaks with sharp knife , discarding as many bones as possible , and re - form the steaks . arrange the steaks in 2 buttered shallow baking pans , ladle the wine mixture around them , and poach the steaks , covered with a buttered piece of foil , on the upper and lower racks of a preheated 375°f . oven , switching the positions of the pans after 15 minutes , for 30 minutes . let the steaks cool , the foil , and chill them in the poaching liquid , covered , for at least 6 hours or overnight . separate the salmon steaks into halves , brush them with some of the poaching liquid , and arrange them on a platter . garnish the salmon with dill sprigs and the lemon slice\n",
      "Target (y): for oven - poached salmon steaks with mustard dill sauce | in a saucepan combine the wine , 1 1 / 2 cups water , the sugar , the peppercorns , the coriander seeds , the mustard seeds , the salt , and the onion , simmer the mixture for 15 minutes , and let cool to room temperature . halve the salmon steaks with sharp knife , discarding as many bones as possible , and re - form the steaks . arrange the steaks in 2 buttered shallow baking pans , ladle the wine mixture around them , and poach the steaks , covered with a buttered piece of foil , on the upper and lower racks of a preheated 375°f . oven , switching the positions of the pans after 15 minutes , for 30 minutes . let the steaks cool , the foil , and chill them in the poaching liquid , covered , for at least 6 hours or overnight . separate the salmon steaks into halves , brush them with some of the poaching liquid , and arrange them on a platter . garnish the salmon with dill sprigs and the lemon slice <pad>\n",
      "---\n",
      "Input (x): recipe for peachy grilled chicken salad | whisk 1 tablespoon olive oil , lime juice , 1 teaspoon salt , and 1 / 2 teaspoon pepper in 11x7 - inch glass dish . add chicken and turn to coat . marinate 30 minutes , turning occasionally . prepare barbecue ( medium - high heat ) . whisk remaining 4 tablespoons oil , 1 / 2 teaspoon salt , 1 / 2 teaspoon pepper , onions , shallot , vinegar , thyme , and mustard in large bowl to blend . mix peaches , avocado , and radicchio into dressing toss to coat . grill chicken until cooked through , about 5 minutes per side . transfer to work surface cut crosswise into thin slices . mix baby greens into dressing in bowl . divide salad among 4 plates . arrange chicken alongside and serve . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Target (y): for peachy grilled chicken salad | whisk 1 tablespoon olive oil , lime juice , 1 teaspoon salt , and 1 / 2 teaspoon pepper in 11x7 - inch glass dish . add chicken and turn to coat . marinate 30 minutes , turning occasionally . prepare barbecue ( medium - high heat ) . whisk remaining 4 tablespoons oil , 1 / 2 teaspoon salt , 1 / 2 teaspoon pepper , onions , shallot , vinegar , thyme , and mustard in large bowl to blend . mix peaches , avocado , and radicchio into dressing toss to coat . grill chicken until cooked through , about 5 minutes per side . transfer to work surface cut crosswise into thin slices . mix baby greens into dressing in bowl . divide salad among 4 plates . arrange chicken alongside and serve . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "---\n",
      "Input (x): recipe for cornmeal guava thumbprint cookies | preheat oven to 350°f . combine all purpose flour , cornmeal , ground cinnamon , nutmeg and salt in medium bowl . using electric mixer , beat unsalted butter and brown sugar in large bowl until fluffy . mix in egg yolk and vanilla extract . mix in dry ingredients . form dough into 1 - inch balls . arrange on ungreased baking sheet , spacing 1 1 / 2 inches apart . make depression in center of each using thumb or handle of wooden spoon . fill depressions with jelly . continue baking until bottoms of cookies are brown , about 10 minutes . cool on rack . ( can be prepared 1 week ahead . refrigerate in airtight container . ) <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Target (y): for cornmeal guava thumbprint cookies | preheat oven to 350°f . combine all purpose flour , cornmeal , ground cinnamon , nutmeg and salt in medium bowl . using electric mixer , beat unsalted butter and brown sugar in large bowl until fluffy . mix in egg yolk and vanilla extract . mix in dry ingredients . form dough into 1 - inch balls . arrange on ungreased baking sheet , spacing 1 1 / 2 inches apart . make depression in center of each using thumb or handle of wooden spoon . fill depressions with jelly . continue baking until bottoms of cookies are brown , about 10 minutes . cool on rack . ( can be prepared 1 week ahead . refrigerate in airtight container . ) <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch from the DataLoader\n",
    "for x, y in text_loader:\n",
    "    break\n",
    "\n",
    "# Convert indices back to tokens\n",
    "def indices_to_tokens(indices):\n",
    "    itos = vocab.get_itos()\n",
    "    return ' '.join([itos[index] for index in indices])\n",
    "\n",
    "# Print the first few sentences in the batch\n",
    "for i in range(5):  # Change this number to print more or fewer sentences\n",
    "    print(\"Input (x):\", indices_to_tokens(x[i]))\n",
    "    print(\"Target (y):\", indices_to_tokens(y[i]))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  25,   16,  477,  ...,    0,    0,    0],\n",
       "         [  25,   16, 1489,  ...,    0,    0,    0],\n",
       "         [  25,   16, 1268,  ...,  174,   63,    6],\n",
       "         ...,\n",
       "         [  25,   16, 2002,  ...,    0,    0,    0],\n",
       "         [  25,   16, 4753,  ...,    0,    0,    0],\n",
       "         [  25,   16,  280,  ...,    0,    0,    0]]),\n",
       " tensor([[  16,  477,  200,  ...,    0,    0,    0],\n",
       "         [  16, 1489,  625,  ...,    0,    0,    0],\n",
       "         [  16, 1268,  382,  ...,   63,    6,    9],\n",
       "         ...,\n",
       "         [  16, 2002,  469,  ...,    0,    0,    0],\n",
       "         [  16, 4753,   26,  ...,    0,    0,    0],\n",
       "         [  16,  280,   13,  ...,    0,    0,    0]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x, y in text_loader:\n",
    "    # x and y are the input and target sequences for this batch\n",
    "    break\n",
    "\n",
    "x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recipe', 'for', 'lentil', ',', 'apple', ',', 'and', 'turkey', 'wrap', '|', '1', '.', 'place', 'the', 'stock', ',', 'lentils', ',', 'celery', ',', 'carrot', ',', 'thyme', ',', 'and', 'salt', 'in', 'a', 'medium', 'saucepan', 'and', 'bring', 'to', 'a', 'boil', '.', 'reduce', 'heat', 'to', 'low', 'and', 'simmer', 'until', 'the', 'lentils', 'are', 'tender', ',', 'about', '30', 'minutes', ',', 'depending', 'on', 'the', 'lentils', '.', '(', 'if', 'they', 'begin', 'to', 'dry', 'out', ',', 'add', 'water', 'as', 'needed', '.', ')', 'remove', 'and', 'discard', 'the', 'thyme', '.', 'drain', 'and', 'transfer', 'the', 'mixture', 'to', 'a', 'bowl', 'let', 'cool', '.', '2', '.', 'fold', 'in', 'the', 'tomato', ',', 'apple', ',', 'lemon', 'juice', ',', 'and', 'olive', 'oil', '.', 'season', 'with', 'the', 'pepper', '.', '3', '.', 'to', 'assemble', 'a', 'wrap', ',', 'place', '1', 'lavash', 'sheet', 'on', 'a', 'clean', 'work', 'surface', '.', 'spread', 'some', 'of', 'the', 'lentil', 'mixture', 'on', 'the', 'end', 'nearest', 'you', ',', 'leaving', 'a', '1', '-', 'inch', 'border', '.', 'top', 'with', 'several', 'slices', 'of', 'turkey', ',', 'then', 'some', 'of', 'the', 'lettuce', '.', 'roll', 'up', 'the', 'lavash', ',', 'slice', 'crosswise', ',', 'and', 'serve', '.', 'if', 'using', 'tortillas', ',', 'spread', 'the', 'lentils', 'in', 'the', 'center', ',', 'top', 'with', 'the', 'turkey', 'and', 'lettuce', ',', 'and', 'fold', 'up', 'the', 'bottom', ',', 'left', 'side', ',', 'and', 'right', 'side', 'before']\n"
     ]
    }
   ],
   "source": [
    "example_index = 0  # Index of the example sentence you want to examine\n",
    "example_tokens = text_ds[example_index]\n",
    "print(example_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return self.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(10000, 100)\n",
      "  (lstm): LSTM(100, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=10000, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(VOCAB_SIZE, EMBEDDING_DIM, N_UNITS)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 4.714126110076904\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn.functional import nll_loss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define the number of epochs\n",
    "EPOCHS = 10  # Replace with your value\n",
    "\n",
    "# Define a function to generate text\n",
    "def generate_text(start_prompt, max_tokens, temperature):\n",
    "    start_tokens = [vocab[token] for token in start_prompt.split()]\n",
    "    for _ in range(max_tokens):\n",
    "        x = torch.tensor([start_tokens]).to(device)\n",
    "        y = model(x)\n",
    "        probs = torch.exp(y[0][-1])  # Convert log probabilities to probabilities\n",
    "        sample_token = torch.multinomial(probs, num_samples=1, replacement=True)\n",
    "        start_tokens.append(sample_token.item())\n",
    "        if sample_token.item() == vocab['<eos>']:\n",
    "            break\n",
    "    generated_text = ' '.join([vocab.get_itos()[token] for token in start_tokens])\n",
    "    print(f\"\\ngenerated text:\\n{generated_text}\\n\")\n",
    "\n",
    "# Move the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "model.to(device)\n",
    "\n",
    "# Create a SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for i, (x, y) in enumerate(text_loader):\n",
    "        # Move the data to the device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y_pred.reshape(-1, VOCAB_SIZE), y.reshape(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {i}, Loss: {loss.item()}')\n",
    "\n",
    "        # Log loss and perplexity to TensorBoard\n",
    "        writer.add_scalar('Loss', loss.item(), epoch * len(text_loader) + i)\n",
    "        writer.add_scalar('Perplexity', torch.exp(loss).item(), epoch * len(text_loader) + i)\n",
    "\n",
    "    # Generate text at the end of each epoch\n",
    "    generate_text(\"recipe for\", max_tokens=100, temperature=1.0)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'lstm_text_genv1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
