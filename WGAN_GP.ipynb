{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.15.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from vae_utils import get_vector_from_label, add_vector_to_images, morph_faces\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "\n",
    "#WEIGHT_CLIP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tt.Compose(\n",
    "    [\n",
    "        tt.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms, download=True)\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"/Users/parkermoesta/datasets/CelebA/img_align_celeba/\", transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channel_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input N x channel_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channel_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # 32x32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.d_block(features_d, features_d*2, 4, 2, 1), # out: (batch_size, features_d*2, 16, 16)\n",
    "            self.d_block(features_d*2, features_d*4, 4, 2, 1), # out: (batch_size, features_d*4, 8, 8)\n",
    "            self.d_block(features_d*4, features_d*8, 4, 2, 1), # out: (batch_size, features_d*8, 4, 4)\n",
    "            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # out: (batch_size, 1, 1, 1)\n",
    "        )\n",
    "\n",
    "    def d_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True), # instance norm instead of batch norm, instancenorm is applied to each image individually\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: N x z_dim x 1 x 1\n",
    "            self.gen_block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4\n",
    "            self.gen_block(features_g*16, features_g*8, 4, 2, 1), # N x f_g*8 x 8 x 8\n",
    "            self.gen_block(features_g*8, features_g*4, 4, 2, 1), # N x f_g*4 x 16 x 16\n",
    "            self.gen_block(features_g*4, features_g*2, 4, 2, 1), # N x f_g*2 x 32 x 32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g*2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ), # N x channels_img x 64 x 64\n",
    "            nn.Tanh(), # [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def gen_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels), # don't need to use bias as batchnorm's learnable parameters\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake):\n",
    "    BATCH_SIZE, C, H, W = real.shape # 64, 3, 64, 64\n",
    "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device) # epsilon will be used to interpolate between real and fake images\n",
    "    interpolated_images = real * epsilon + fake * (1 - epsilon) # interpolated_images will be used to calculate the gradient penalty\n",
    "\n",
    "    mixed_scores = critic(interpolated_images) # mixed_scores will be used to calculate the gradient penalty\n",
    "\n",
    "    gradient = torch.autograd.grad( # gradient will be used to calculate the gradient penalty\n",
    "        inputs=interpolated_images, # interpolated_images is the input to calculate the gradient\n",
    "        outputs=mixed_scores, # mixed_scores is the output to calculate the gradient\n",
    "        grad_outputs=torch.ones_like(mixed_scores), # torch.ones_like(mixed_scores) is the gradient of the output\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0] # [0] to get the first element of the tuple returned by torch.autograd.grad\n",
    "\n",
    "    gradient = gradient.view(gradient.shape[0], -1) # flatten the gradient tensor\n",
    "    gradient_norm = gradient.norm(2, dim=1) # calculate the norm of the gradient tensor\n",
    "\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "# need to initialize weights on generator and discriminator\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers \n",
    "#opt_gen = torch.optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
    "#opt_critic = torch.optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = torch.optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "\n",
    "# loss function\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 0/3166                   Loss D: 627.7452, loss G: 0.0963\n"
     ]
    }
   ],
   "source": [
    "# setting fixed noise for visualization\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "\n",
    "step = 0 # for printing to tensorboard\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real,_) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(real).reshape(-1) # flatten\n",
    "            critic_fake = critic(fake).reshape(-1) # flatten\n",
    "            gp = gradient_penalty(critic, real, fake)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp)\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True) # retain_graph=True to prevent error\n",
    "            opt_critic.step()\n",
    "\n",
    "\n",
    "        ## Train Generator: min -E[critic(gen_fake)]\n",
    "        output = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(output) # we want to minimize the loss\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        # print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
